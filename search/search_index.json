{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"How To Get Rid of Secrets in Kubernetes with Workload Identity Federation","text":"<p>When developing Software, we're interacting with secrets during all stages of development.  In our applications, in CI/CD pipelines, and sometimes even for local development, if unavoidable.  While we can take measures to limit the chance of leaking a secret, it is never zero.  Good security practises and tools such as detect-secrets or talisman  can protect against some scenarios, but are not perfect. When a secret gets exposed,  the impact highly depends on a few of its properties:</p> <ul> <li>Being shared / being shareable: Rotating shared secrets can be expensive or take time, as multiple parties might need to update it.</li> <li>Being nonpersonalized: Access logs can be useful to investigate security issues. However, if it can not be determined who accessed an asset because the credentials are not tied to an identity, an audit trail might be harder to follow.</li> <li>Being long-lived: In many cases, the fact that a secret has been exposed  will not become apparent for a long time. If it doesn't expire and doesn't get rotated often,  a potential attacker has a long window of opportunity to exploit the leak.</li> <li>Being too powerful: If a secret provides access to much more data/assets than necessary,  a leak can be much more severe. </li> </ul> <p>An example of a terrible secret is the account key for an azure storage account. There are only two keys available for each account, one of which is supposed to be used at a time, while the other can be used for a smooth rotation. The credentials can not be limited in power, are not bound to an identity and do not expire, ticking most of the boxes in the list above. When one is exposed, all data on the account is available to an attacker (can be petabytes). While some services exclusively support them as an access method, they are actively discouraged by Microsoft. Microsoft instead recommends using role assignments and other identity based authentication options.</p>"},{"location":"#identity-based-access-for-automated-workloads","title":"Identity based Access for automated Workloads","text":"<p>Classically, to enable identity based access for kubernetes workloads, such as application pods or jobs,  an identity is created as a cloud resource. Examples of such identities are Azure Service Principals or GCP Service Accounts. Then, a secret is generated and stored where the identity is used. In Kubernetes, it is usually stored in a Kubernetes Secret,  and injected into the workload. Together with solid security practices within Kubernetes, like the use of Kubernetes RBAC (Role based Access Control) and namespace isolation,  this is already a step up from using anonymous credentials. However, these secrets are usually still quite long-lived and need to be rotated (and therefore moved) regularly. Luckily, there is a way to completely get rid of those secrets for good and instead only rely on short-lived tokens for many common scenarios.</p>"},{"location":"#workload-identity-federation-for-kubernetes","title":"Workload Identity Federation for Kubernetes","text":"<p>On a high level, workload identity federation allows the identity provider in the cloud (the one issuing tokens for GCP service accounts) to trust the identity provider in kubernetes (the one issuing tokens for kubernetes service accounts),  so that an application running in a kubernetes pod can exchange a token for it's service account for a token for the service account in the cloud.</p> <p></p> <p>Here's some example terraform code to create a cloud identity (user assigned identity on azure, service account on GCP) along with the association to a kubernetes service account, identified by the namespace and service account name.</p> AzureGCP <pre><code>resource \"azurerm_user_assigned_identity\" \"example\" {\nlocation            = \"west-europe\"\nname                = \"&lt;some-cloud-identity-name&gt;\"\nresource_group_name = \"&lt;resource-group-name&gt;\"\n}\n\nresource \"azurerm_federated_identity_credential\" \"example\" {\nname                = \"example\"\nresource_group_name = \"&lt;resource-group-name&gt;\"\naudience            = [\"api://AzureADTokenExchange\"]\nissuer              = \"https://oidc.prod-aks.azure.com/&lt;cluster-issuer-id&gt;/\"\nparent_id           = azurerm_user_assigned_identity.example.id\nsubject             = \"system:serviceaccount:&lt;namespace-name&gt;:&lt;service-account-name&gt;\"\n}\n</code></pre> <pre><code>resource \"google_service_account\" \"example\" {\naccount_id   = \"&lt;some-cloud-identity-name&gt;\"\ndisplay_name = \"My GCP cloud identity\"\nproject      = var.project_id\n}\n\nresource \"google_service_account_iam_member\" \"main\" {\nservice_account_id = google_service_account.example.account_id\nrole               = \"roles/iam.workloadIdentityUser\"\nmember             = \"serviceAccount:${var.project_id}.svc.id.goog[&lt;namespace-name&gt;/&lt;service-account-name&gt;]\"\n}\n</code></pre> <p>Warning</p> <p>Note that this configuration does not reference the name of the GKE cluster, and will equally apply to all GKE clusters in a project. An attacker could take advantage of this if one cluster is not protected sufficiently (for example when exploring or migrating).</p> <p>On kubernetes side, a pod using workload identity can be configured as usual, with a few additions:</p> AzureGCP <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nannotations:\nazure.workload.identity/client-id: &lt;client-id-of-azure-identity&gt;\nname: &lt;service-account-name&gt;\nnamespace: &lt;namespace&gt;\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: &lt;some-pod-name&gt;\nnamespace: &lt;namespace&gt;\nlabels:\nazure.workload.identity/use: \"true\"\nspec:\nserviceAccountName: &lt;service-account-name&gt;\ncontainers:\n- ...\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nannotations:\niam.gke.io/gcp-service-account: &lt;cloud-identity-name&gt;@&lt;project-id&gt;.iam.gserviceaccount.com\nname: &lt;service-account-name&gt;\nnamespace: &lt;namespace&gt;\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: &lt;some-pod-name&gt;\nnamespace: &lt;namespace&gt;\nspec:\nserviceAccountName: &lt;service-account-name&gt;\nnodeSelector:\niam.gke.io/gke-metadata-server-enabled: \"true\" #Needed if workload identity is not enabled on all nodepools\ncontainers:\n- ...\n</code></pre> <p>If these resources are deployed to a cluster that has workload identity enabled,  additional environment variables and a volume containing a token will be injected into the pod.</p> <p>At this point, using workload identities becomes very straightforward if the client libraries provided by  the cloud providers already support it. Here is a few examples accessing cloud resources:</p> Azure Key VaultGCP Secret Manager <pre><code>from azure.keyvault.secrets import SecretClient\nfrom azure.identity import DefaultAzureCredential\n\ndef main():\n    credential = DefaultAzureCredential()\n    keyvault_client = SecretClient(vault_url=\"&lt;some-keyvault-url&gt;\", credential=credential)\n    secret = keyvault_client.get_secret(\"&lt;some-secret-name&gt;\")\n    do_something_with_secret(secret.value)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <pre><code>from google.cloud import secretmanager\n\ndef main():\n    client = secretmanager.SecretManagerServiceClient()\n    name = f\"projects/&lt;project_id&gt;/secrets/&lt;secret_id&gt;/versions/latest\"\n    response = client.access_secret_version(request={\"name\": name})\n    secret_value = response.payload.data.decode(\"UTF-8\")\n    do_something_with_secret(secret_value)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>As visible, both Azure and GCP provide good support to hide the complexity that comes with workload identity.</p>"},{"location":"#what-if-secrets-are-unavoidable","title":"What if secrets are unavoidable?","text":"<p>While it is relatively easy to access resources in the own cloud provider using workload identities, external dependencies often don't offer the possibility. For example, a monitoring platform like Datadog might only offer authentication using api keys.  Even in this case, workload identities offer some advantages in conjunction with cloud secret stores. Like in the previous code example, an external secret can be stored in a secret manager and accessed from the pod directly, circumventing the need to create kubernetes secrets from a pipeline and injecting it into the pod via environment variables.</p> <p>If the component needing access to the secret is not a custom application, there are CSI Drivers available to sync secrets from secrets stores into kubernetes (as mounted volumes or additionally as kubernetes secrets). However, there are security tradeoffs when compared with direct integration via an API.</p>"},{"location":"#what-if-components-can-only-work-with-long-term-credentials","title":"What if components can only work with long-term credentials?","text":"<p>Sometimes, when deploying generic components without custom code, it is not straightforward to work with tokens instead of long-lived secrets. For example, a service like Backstage might depend on a Postgres database as a backend. While postgres in Azure and GCP both support identity based auth, the tool in question might only allow configuration of a fixed username/password. Here are a few approaches how to approach this issue:</p>"},{"location":"#contribute-to-open-source","title":"Contribute to open source","text":"<p>Sometimes, it is easiest to raise a pull-request with an open source project, than to find workarounds to address limited support. That could mean adding the ability to configure a function instead of a static configuration value to fetch a password, enabling a backend to function with default credentials for major cloud providers or just adding a missing setting to a helm chart. This has the added benefit of contributing to the community and also saves maintenance efforts for custom adaptions. In many cases, it can also be enough to check and vote for an existing issue or to raise a new one, if a direct contribution is unrealistic.</p>"},{"location":"#sidecars-refreshers-and-proxies","title":"Sidecars, Refreshers and Proxies","text":"<p>It is often possible to add sidecars to containers that update credentials.  In the case of postgres, they could fetch a credential in regular intervals and share it with the other container via a shared volume. Whether this works or not might still depend, as not every component will react to changes at runtime. A more drastic option is a component that forces a pod recreation whenever a secret expires or is about to expire. If the kubernetes resources are configured appropriately, that would not have any impact on availability. Another option is to put a proxy between the application and the external dependency it is trying to access. For Google Cloud SQL, there is an option available  that can be deployed within a namespace or as a sidecar.</p> <p>Tip</p> <p>Something to keep in mind when considering these options is whether the benefits for security actually outweigh the additional architectural complexity.  Thread Modelling can help with prioritisation.</p>"},{"location":"#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Secrets are omnipresent and important in software, but can pose a risk if they are weak, long-lived, powerful or shared or not identity-based</li> <li>Everytime a secret is moved between places, there is a risk of exposing it, so movement should be minimized</li> <li>Workload Identity Federation, along with fine-grained permissions, is a powerful way to eliminate persistent secrets</li> </ul>"},{"location":"#related-resources","title":"Related Resources","text":"<p>Azure Workload Identity</p> <p>GKE Workload Identity</p> <p>AWS Pod Identity</p>"}]}